{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wMaAY7y5jIZE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install torch>=2.1.0 transformers>=4.36.0 datasets>=2.15.0\n",
        "!pip install accelerate>=0.24.0 peft>=0.7.0 trl>=0.7.0\n",
        "!pip install bitsandbytes>=0.41.3 wandb>=0.16.0 xformers\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1XsiyYz-7hbw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade transformers torch torchvision unsloth\n",
        "!pip install --upgrade datasets accelerate peft trl bitsandbytes wandb xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "beccc372db244c66bf4b47b0915b6776",
            "3474753fe177403ebd395e20d1fe5a68",
            "7bdc50a725224ee88a5edb5d97db478e",
            "d827ce294000492fb726c45f3936021a",
            "bf0750a88c56428780dc0451aaabc113",
            "47f3aae179fb4f12b20aa13014cfa7c4",
            "7166c8b9a9e34671aa4ed84120d6f1f9",
            "6401b1b6f0654f60a09951cde249f48a",
            "5d31f1dd24fb4abe962947e3ece5335e",
            "04295f16add644cf835ac31c60aeae07",
            "f96dc1772ea6430a90143bbccf13c632",
            "6677e082cf2c4812a762290a2af94e45",
            "7d8100afb2364b1488499625fd10cd80",
            "dccddca3407a4776a0f18c7c8b54d5a7",
            "a8d76ddf7d304ba998c11118a7df64b2",
            "7badc48d11ca45bd9879c4e7aa89203b",
            "1c3f34a2fa9d490a9eda0d3173bf23b4",
            "789162fb373844c291cfcf7f574d8753",
            "68b4c27ee4f843f69435392ffa9a26e5",
            "9c89df87a3e543858d7878fdfc46c44d"
          ]
        },
        "id": "jZJgxMQilKrB",
        "outputId": "79119b0d-e8c1-45a2-c7fc-f76a0c0b5244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë Please login to HuggingFace Hub:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beccc372db244c66bf4b47b0915b6776",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë Please login to Wandb:\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeera\u001b[0m (\u001b[33mkeera-nanyang-technological-university-singapore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Authentication complete!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "import wandb\n",
        "print(\"üîë Please login to HuggingFace Hub:\")\n",
        "notebook_login()\n",
        "print(\"üîë Please login to Wandb:\")\n",
        "wandb.login()\n",
        "print(\"‚úÖ Authentication complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbe6pPbIlMnj",
        "outputId": "30bcf2f1-8737-4df1-bb3b-77480ea21c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Setting up Google Colab environment...\n",
            "‚úÖ GPU: NVIDIA A100-SXM4-40GB\n",
            "‚úÖ GPU Memory: 42.5 GB\n",
            "üöÄ A100 GPU detected - using optimized settings for Llama 3.1!\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted and output directory created: /content/drive/MyDrive/llama_31_therapist_outputs\n",
            "‚úÖ Environment setup complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"üîß Setting up Google Colab environment...\")\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name()\n",
        "    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU: {device_name}\")\n",
        "    print(f\"‚úÖ GPU Memory: {memory_gb:.1f} GB\")\n",
        "\n",
        "    # Optimize settings based on GPU for Llama 3.1 8B\n",
        "    if \"A100\" in device_name:\n",
        "        print(\"üöÄ A100 GPU detected - using optimized settings for Llama 3.1!\")\n",
        "        batch_size = 4  # Conservative for 8B model stability\n",
        "        max_seq_length = 2048  # Standard for Llama 3.1 8B\n",
        "    elif \"V100\" in device_name:\n",
        "        print(\"‚ö° V100 GPU detected - using conservative settings\")\n",
        "        batch_size = 2\n",
        "        max_seq_length = 1024\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Other GPU detected - using very conservative settings\")\n",
        "        batch_size = 1\n",
        "        max_seq_length = 512\n",
        "else:\n",
        "    print(\"‚ùå No GPU available!\")\n",
        "    raise RuntimeError(\"This notebook requires a GPU!\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "output_dir = '/content/drive/MyDrive/llama_31_therapist_outputs'\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"‚úÖ Google Drive mounted and output directory created: {output_dir}\")\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr7yQB6ylRIL",
        "outputId": "26e026dc-9f52-4de6-b5a4-47a82a3d6e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Configuration loaded for Llama 3.1 8B Instruct training!\n",
            "üéØ Model: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
            "üéØ Training dataset: ShenLab/MentalChat16K\n",
            "üéØ Testing dataset: NickyNicky/nlp-mental-health-conversations\n",
            "üéØ Effective batch size: 16\n",
            "üéØ Max sequence length: 2048\n",
            "üéØ Learning rate: 0.0002\n"
          ]
        }
      ],
      "source": [
        "CONFIG = {\n",
        "    # Model settings - Llama 3.1 8B Instruct specific\n",
        "    'model_name': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit',\n",
        "    'max_seq_length': max_seq_length,\n",
        "    'load_in_4bit': True,\n",
        "\n",
        "    # LoRA settings optimized for Llama 3.1 8B\n",
        "    'lora_r': 16,  # Good balance for 8B model\n",
        "    'lora_alpha': 16,  # Matches rank for stability\n",
        "    'lora_dropout': 0.0,  # No dropout for stable training\n",
        "    'lora_target_modules': [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "\n",
        "    # Dataset settings\n",
        "    'train_dataset_name': 'ShenLab/MentalChat16K',  # For training/validation\n",
        "    'test_dataset_name': 'NickyNicky/nlp-mental-health-conversations',  # For testing\n",
        "    'validation_split': 0.1,\n",
        "\n",
        "    # Training hyperparameters - Conservative for slow stable convergence\n",
        "    'per_device_train_batch_size': batch_size,\n",
        "    'per_device_eval_batch_size': batch_size,\n",
        "    'gradient_accumulation_steps': 4,  # Effective batch size = batch_size * 4\n",
        "    'warmup_steps': 5,  # Very gradual warmup\n",
        "    'num_train_epochs': 3,  # 3 epochs should be sufficient\n",
        "    'learning_rate': 2e-4,  # Conservative learning rate for stability\n",
        "    'weight_decay': 0.01,  # L2 regularization\n",
        "    'lr_scheduler_type': 'cosine',  # Smooth decay\n",
        "\n",
        "    # Logging and saving - More frequent for monitoring\n",
        "    'logging_steps': 5,\n",
        "    'eval_steps': 50,\n",
        "    'save_steps': 50,\n",
        "    'save_total_limit': 2,\n",
        "    'output_dir': output_dir,\n",
        "\n",
        "    # Wandb settings\n",
        "    'use_wandb': True,\n",
        "    'wandb_project': 'llama-31-therapist',\n",
        "    'wandb_entity': None,  # Set to your wandb username if needed\n",
        "\n",
        "    # Generation settings for inference\n",
        "    'generation_config': {\n",
        "        'max_new_tokens': 512,\n",
        "        'temperature': 0.7,\n",
        "        'top_p': 0.9,\n",
        "        'repetition_penalty': 1.1,\n",
        "        'do_sample': True,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìã Configuration loaded for Llama 3.1 8B Instruct training!\")\n",
        "print(f\"üéØ Model: {CONFIG['model_name']}\")\n",
        "print(f\"üéØ Training dataset: {CONFIG['train_dataset_name']}\")\n",
        "print(f\"üéØ Testing dataset: {CONFIG['test_dataset_name']}\")\n",
        "print(f\"üéØ Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"üéØ Max sequence length: {CONFIG['max_seq_length']}\")\n",
        "print(f\"üéØ Learning rate: {CONFIG['learning_rate']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455,
          "referenced_widgets": [
            "f7ba83587c3440e9a659c77c79631c3c",
            "9e67b0a7f7d542c0a1a1861c4c63dab8",
            "1e2c41ad758b425abd94694c629ece73",
            "4286a083cf5342b49940f78f1ae172ad",
            "2015ba4aab974f0a9b4b3c38d0f0e0bc",
            "7a3567ac9d414e7ab83dbcd899246424",
            "1c697ead2d084ac1ad59fee765eb2b3f",
            "f68ff9065a1146aab90d178fdbcd76ac",
            "294b8a11bf3d487c92222d61b1dfabfd",
            "450d325de067464bb0f8dd6654c7bfb9",
            "f09e057ddcd547eaa1b7c1bac11c07db",
            "42324dd7e5bc4709bbe87e6f3e7b59c5",
            "c8610d0d3e204b4c996ce8848d118b30",
            "8d15b5f93c9845399d74c88e84a3b7d1",
            "9ee8e80e58ff46d1b9f89babfc2ece41",
            "d65b934df69649968fa3280095eed160",
            "214c04285fb64a2db098e83448f7e4ca",
            "38e08cfcefd1438fbbcc2ab9d2d218e1",
            "9dcc740f71ad4ef199545606e17036df",
            "43765149bb184437999fe44e99d24312",
            "96039a72d2f748db991ad3ab3c5afe2c",
            "7781d72e23ef4664911cbdc63edf8bee",
            "953af0741c194a30a77935f6fd1a91f2",
            "2d6ff88d8cc9401f8a50fed846e0e1f4",
            "c4c1fcffbc80491b897f5bf5d5d4b2e4",
            "aae9e37ac1cd424a8f63b7d191406c7e",
            "5a4405cfdb544d73835578ac0af080b5",
            "f00faa09c4144df7999e9d5ef813c147",
            "3ff8b9dbc1544dd49821cca13c145abe",
            "8ff3720d8e284a12ac347476811bc712",
            "1e104b3cbbbb463fa3a6dd8063ddec70",
            "916a34a103c2409abec254d1373850d8",
            "75ffb6c2e151432989d2572529240023",
            "3d24fb5cee82412281b1a4bb429cb5ca",
            "1c79d9e84bdd4439a3093c00c8412f10",
            "4413e9e1a9044904b604375423e34025",
            "f3722a148afe4cd98dfab383c53349df",
            "196b7f0765414fd5a75949d2d19f654b",
            "a7cc59e92ebf49c2aff49e96b5885a1a",
            "937aacfe065849ee8021acf9ca4ffa85",
            "fab4e3b64f85471fb609ae13834ba4db",
            "24c194f04c494171b320bb2660172dc8",
            "a98e778d75a54d299ae73d745e2b8963",
            "64de7646d52f4921b1858604d1317640",
            "812a3bcbc3d94cb8903d1d93a1752a94",
            "c104691cf8194d1b9ad9377c2ee5b4a6",
            "c568496a4d0d4ce49ea72de80357ae6b",
            "b056b7571f4e43bbb66465f7df68575e",
            "43aea0421c7441cf86f019c28cf8195c",
            "a59a325f3a6648709acc7e8f8da7d977",
            "ba9bfe81646f4d4d9f69735ff96781ab",
            "97cd1568284349108141d7f70a5801ac",
            "f7c4ce318f53418b8c43ab4b0bd22af1",
            "367e98fb1b3543c495c5dca8e1c0d6a6",
            "110afa85ffac4cb78133f5a62ce4cda4"
          ]
        },
        "id": "WsJLO1vSlUmQ",
        "outputId": "4e646e4e-2f98-41e2-db90-cb7fc031b33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "ü¶ô Loading Llama 3.1 8B Instruct model with Unsloth optimizations...\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.53.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7ba83587c3440e9a659c77c79631c3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42324dd7e5bc4709bbe87e6f3e7b59c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "953af0741c194a30a77935f6fd1a91f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d24fb5cee82412281b1a4bb429cb5ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "812a3bcbc3d94cb8903d1d93a1752a94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.6.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Llama 3.1 8B Instruct model loaded successfully!\n",
            "üìä Total parameters: 4.6B\n",
            "üìä Trainable parameters: 41,943,040 (0.92%)\n",
            "üìä Memory efficient: 4-bit quantization enabled\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "\n",
        "print(\"ü¶ô Loading Llama 3.1 8B Instruct model with Unsloth optimizations...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=CONFIG['model_name'],\n",
        "    max_seq_length=CONFIG['max_seq_length'],\n",
        "    dtype=None,  # Auto-detect best dtype\n",
        "    load_in_4bit=CONFIG['load_in_4bit'],\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters specifically configured for Llama 3.1\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=CONFIG['lora_r'],\n",
        "    target_modules=CONFIG['lora_target_modules'],\n",
        "    lora_alpha=CONFIG['lora_alpha'],\n",
        "    lora_dropout=CONFIG['lora_dropout'],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # Standard LoRA for stability\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"‚úÖ Llama 3.1 8B Instruct model loaded successfully!\")\n",
        "print(f\"üìä Total parameters: {total_params / 1e9:.1f}B\")\n",
        "print(f\"üìä Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"üìä Memory efficient: 4-bit quantization enabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "531ecbc18944472d990fe763a06817ad",
            "6543f9393ca64b54b3b847e459eafefa",
            "a719a7b6b2074f8ea22f6d260995d268",
            "4eea0e9c62844754b8945376f4946614",
            "7a53f0ea7d19481880897fd1c12240a1",
            "e6d64e52bd0d4d5699beaae747db30f6",
            "6250ee5eb58c48759872b3fa0ad0855d",
            "8dc7891f705e44a3988be03dbbb26c73",
            "9bb9e855e4f64aa4a902706b8b61c8cf",
            "50efd44489514a8aa286836bbe99fc69",
            "1e5cfb2ee7a341eebe2e1d3a06258986",
            "b6aadf74c7794799bd920e4f8b56f752",
            "ea7c9a6bbaf34791b236a980dc47b774",
            "fe14845826f74731a2b5190637915dbc",
            "b29d2912e2484ccbacd66e90910d897d",
            "bc48ad9d9f404678b72ac2cafc99b1dd",
            "ab643005e3fd4ae8a30507e80a2df4ce",
            "ac6d0019a10d427092422c8c3f2357a1",
            "435fc19265494a95ae24cb185dd5d8f8",
            "929098be955d405fabdb59776957fd05",
            "9a3639a2ef8041539390d74a3b4d0c77",
            "3b4104443a8c4912b3fd32d9e0986490",
            "81281a975e0445bdbb70bb744bcae047",
            "d21e6c41811942a58132f8582c73a174",
            "9437e270d43f4baebb00731204e7aa63",
            "4b16cedc12934894b2b288c5bd1ebb38",
            "91e546d2caf94e1490f83adb25b22bfc",
            "8b5eb74188864ec4918e5bdc7d763db5",
            "8fd6ca8e79be482c8aef44aadb01d2fb",
            "ef39d3a573fb48b9839354aba366756e",
            "1fe7d12b1d154982afcf6f9a1093d059",
            "bd70a82c60c64fda9a776ed81f1a72b0",
            "c7268555908f44f7ac4946f1c23ba08e",
            "60318e8c1d28439aba8335697df7ab50",
            "31db3c3f52d0453fb5e44c16f06e8299",
            "fb68facc2526454eac4c4859c9ec7414",
            "7f3a3024fdb044369f1033f5d0673dfa",
            "2d51800f4f764fffacc52792d30c1540",
            "e2b231c6d4b34ea29a0b7424dbd095b7",
            "393604410b874c14b763782dc27e2152",
            "4a4517da5c764735aa2f5c2c4bffd5f8",
            "62a9bc414f7349fabf5e8793a9fcaf6e",
            "a9a41ae7f9ef42c2a47dbb64bb3cf407",
            "73b9219f6d854703a06524dae1f6a028",
            "ffc81b0c60fd4a77b77ecfdc685873b5",
            "24799e7a674245dd8df3b7699d408452",
            "cf1a8327748d4aaa82e477793f8354e2",
            "1618345601d24f9abcef14f2fc3ee04e",
            "bba16e03bc194eda87707b10f1d6bd3c",
            "70e98738fbc548479c9a7ed1f23a0feb",
            "9124c6c6c4bd492f88d54e309346c67f",
            "0044c1bde20b4da8b22fa12f9826a47e",
            "2ab473b80bd647558d71edfd6cada1a7",
            "37b531d2d0d142aabdd49e052771f3f4",
            "c5262161fc724b568b44e4577d724638"
          ]
        },
        "id": "w0_HBzghlXxW",
        "outputId": "a7fe0c26-f1c6-47a4-fc38-f2c99cd83d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Loading ShenLab/MentalChat16K dataset for training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "531ecbc18944472d990fe763a06817ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6aadf74c7794799bd920e4f8b56f752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Interview_Data_6K.csv:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81281a975e0445bdbb70bb744bcae047",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Synthetic_Data_10K.csv:   0%|          | 0.00/32.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60318e8c1d28439aba8335697df7ab50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/16084 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Formatting training conversations for Llama 3.1 Instruct...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffc81b0c60fd4a77b77ecfdc685873b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Formatting training data for Llama 3.1:   0%|          | 0/16084 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training dataset prepared for Llama 3.1!\n",
            "üìä Training samples: 14,475\n",
            "üìä Validation samples: 1,609\n",
            "üìä Total training samples: 16,084\n",
            "\n",
            "üìÑ Sample training conversation format:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful, empathetic mental health assistant. You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description. \n",
            "The assistant gives helpful, comprehensive, and ap...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üìö Loading ShenLab/MentalChat16K dataset for training...\")\n",
        "\n",
        "train_dataset = load_dataset(CONFIG['train_dataset_name'])\n",
        "\n",
        "def format_training_conversation_llama31(example):\n",
        "    \"\"\"Format ShenLab/MentalChat16K conversation for Llama 3.1 Instruct training\"\"\"\n",
        "    # Safely handle missing fields by replacing None with empty strings\n",
        "    instruction = (example.get('instruction') or '').strip()\n",
        "    input_text = (example.get('input') or '').strip()\n",
        "    output = (example.get('output') or '').strip()\n",
        "\n",
        "    # Llama 3.1 Instruct chat format for training\n",
        "    if input_text:\n",
        "        # When there's both instruction and input, use system + user format\n",
        "        conversation = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful, empathetic mental health assistant. {instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{output}<|eot_id|><|end_of_text|>\"\n",
        "    else:\n",
        "        # When only instruction, treat it as user message\n",
        "        conversation = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful, empathetic mental health assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{output}<|eot_id|><|end_of_text|>\"\n",
        "\n",
        "    return {\"text\": conversation}\n",
        "\n",
        "# Process training dataset with Llama 3.1 formatting\n",
        "print(\"üîÑ Formatting training conversations for Llama 3.1 Instruct...\")\n",
        "formatted_train_dataset = train_dataset['train'].map(\n",
        "    format_training_conversation_llama31,\n",
        "    remove_columns=train_dataset['train'].column_names,\n",
        "    desc=\"Formatting training data for Llama 3.1\"\n",
        ")\n",
        "\n",
        "# Create train/validation split\n",
        "dataset_split = formatted_train_dataset.train_test_split(\n",
        "    test_size=CONFIG['validation_split'],\n",
        "    seed=42\n",
        ")\n",
        "train_dataset = dataset_split['train']\n",
        "eval_dataset = dataset_split['test']\n",
        "\n",
        "print(f\"‚úÖ Training dataset prepared for Llama 3.1!\")\n",
        "print(f\"üìä Training samples: {len(train_dataset):,}\")\n",
        "print(f\"üìä Validation samples: {len(eval_dataset):,}\")\n",
        "print(f\"üìä Total training samples: {len(train_dataset) + len(eval_dataset):,}\")\n",
        "\n",
        "# Show a sample formatted for Llama 3.1\n",
        "print(\"\\nüìÑ Sample training conversation format:\")\n",
        "print(train_dataset[0]['text'][:300] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f9eaa3b85534e8080dff826850b95a4",
            "e11fe28f79b5476c87cba4fbe639f489",
            "6bbad5cc395a4dd7bfc7bbd0664bfba4",
            "19ae230dbc28464185ddc197cc7a995d",
            "931e2e01634948a587089a7c1fb8faf5",
            "d934f48b73e74f8fb8adf73b5a17974e",
            "71b16c529fcd4aa4a5fc90f68595ee64",
            "5ea046c65f1b45da84dfa7cd99c3d193",
            "3ec55877908e4b4c849f6370492b8691",
            "897daf547be140ebb90d43f5bf22fbf2",
            "7e8c8908b6674b0296ca9a1b084c32c7",
            "d04b7e4234cd468d982b0dd1981ecd96",
            "2a7a9e08b6b840c1b3df3868bc508711",
            "608ca340ee604fa1b6be53d4883f0b2a",
            "647ae7493d6540c2aef3d683419e31ce",
            "27c5978328c74e2bb4777e8e475a9a3e",
            "4fd81e19b861463a824c51316eac2d22",
            "55364be965ba4a769f33eeb5e694fad8",
            "2199d62689e14e22936379e78392de42",
            "c5f5e7f6fd3848d5a502809ab378f8f0",
            "8f20a3d696e44c958f5deaff4f9f5cdf",
            "92fbe2966ad34d03b89a7a0e850aa90f"
          ]
        },
        "id": "Cc75CMkmldLK",
        "outputId": "dea13dde-57cc-4f5f-812b-4f2c28a37693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è Setting up training for Llama 3.1 8B Instruct...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250629_045751-wgxi9stg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/keera-nanyang-technological-university-singapore/llama-31-therapist/runs/wgxi9stg' target=\"_blank\">llama31-8b-therapist-0629-0457</a></strong> to <a href='https://wandb.ai/keera-nanyang-technological-university-singapore/llama-31-therapist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/keera-nanyang-technological-university-singapore/llama-31-therapist' target=\"_blank\">https://wandb.ai/keera-nanyang-technological-university-singapore/llama-31-therapist</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/keera-nanyang-technological-university-singapore/llama-31-therapist/runs/wgxi9stg' target=\"_blank\">https://wandb.ai/keera-nanyang-technological-university-singapore/llama-31-therapist/runs/wgxi9stg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Wandb initialized: llama31-8b-therapist-0629-0457\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f9eaa3b85534e8080dff826850b95a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/14475 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d04b7e4234cd468d982b0dd1981ecd96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/1609 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training setup complete for Llama 3.1 8B!\n",
            "üìä Effective batch size: 16\n",
            "üìä Total training steps: 2,712\n",
            "üìä Estimated training time: ~135 minutes\n",
            "üìä Using precision: BF16\n",
            "üìä Memory optimization: 4-bit + LoRA\n",
            "\n",
            "üöÄ Starting Llama 3.1 8B Instruct training...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 14,475 | Num Epochs = 3 | Total steps = 2,715\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2715' max='2715' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2715/2715 5:07:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.910200</td>\n",
              "      <td>0.916632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.846000</td>\n",
              "      <td>0.865038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.843200</td>\n",
              "      <td>0.840775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.834000</td>\n",
              "      <td>0.827225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.873200</td>\n",
              "      <td>0.815737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.807300</td>\n",
              "      <td>0.809977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.794600</td>\n",
              "      <td>0.801039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.776400</td>\n",
              "      <td>0.793560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.809900</td>\n",
              "      <td>0.788964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.813600</td>\n",
              "      <td>0.784226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.791500</td>\n",
              "      <td>0.780256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.790400</td>\n",
              "      <td>0.776586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.798100</td>\n",
              "      <td>0.773175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.781500</td>\n",
              "      <td>0.769412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.816800</td>\n",
              "      <td>0.766991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.748200</td>\n",
              "      <td>0.764612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.797400</td>\n",
              "      <td>0.761682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.792700</td>\n",
              "      <td>0.758176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.742100</td>\n",
              "      <td>0.759813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.701300</td>\n",
              "      <td>0.758101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.712400</td>\n",
              "      <td>0.756423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.681900</td>\n",
              "      <td>0.754904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.717400</td>\n",
              "      <td>0.751390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.712800</td>\n",
              "      <td>0.750470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.722200</td>\n",
              "      <td>0.747706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.713300</td>\n",
              "      <td>0.746399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.746500</td>\n",
              "      <td>0.744826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.701000</td>\n",
              "      <td>0.742878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.742511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.703400</td>\n",
              "      <td>0.740108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.680300</td>\n",
              "      <td>0.738253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.733900</td>\n",
              "      <td>0.737228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.700200</td>\n",
              "      <td>0.734277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.706700</td>\n",
              "      <td>0.732511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.692200</td>\n",
              "      <td>0.731396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.724600</td>\n",
              "      <td>0.730553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.638000</td>\n",
              "      <td>0.735670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.651800</td>\n",
              "      <td>0.736912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.649500</td>\n",
              "      <td>0.735959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.620500</td>\n",
              "      <td>0.735287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.627200</td>\n",
              "      <td>0.735700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.616000</td>\n",
              "      <td>0.735810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.653200</td>\n",
              "      <td>0.733460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.639500</td>\n",
              "      <td>0.733301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.658300</td>\n",
              "      <td>0.734525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.628400</td>\n",
              "      <td>0.733057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.604000</td>\n",
              "      <td>0.732272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.646000</td>\n",
              "      <td>0.732079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.682200</td>\n",
              "      <td>0.731735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.633300</td>\n",
              "      <td>0.731466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.625700</td>\n",
              "      <td>0.731408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.648400</td>\n",
              "      <td>0.731325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.637600</td>\n",
              "      <td>0.731381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.604900</td>\n",
              "      <td>0.731393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üéâ Llama 3.1 8B training completed successfully!\n",
            "üìä Final training loss: 0.7210\n",
            "‚è±Ô∏è Training time: 307.9 minutes\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from trl import SFTTrainer\n",
        "import gc\n",
        "import torch\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚öôÔ∏è Setting up training for Llama 3.1 8B Instruct...\")\n",
        "\n",
        "# Initialize wandb with Llama 3.1 specific naming\n",
        "if CONFIG['use_wandb']:\n",
        "    run_name = f\"llama31-8b-therapist-{datetime.now().strftime('%m%d-%H%M')}\"\n",
        "    wandb.init(\n",
        "        project=CONFIG['wandb_project'],\n",
        "        entity=CONFIG.get('wandb_entity'),\n",
        "        name=run_name,\n",
        "        config=CONFIG,\n",
        "        tags=['llama3.1', '8b', 'instruct', 'therapy', 'unsloth', 'a100'],\n",
        "        notes=\"Fine-tuning Llama 3.1 8B Instruct: Train on ShenLab, Test on NickyNicky\"\n",
        "    )\n",
        "    print(f\"‚úÖ Wandb initialized: {run_name}\")\n",
        "\n",
        "# Setup training arguments optimized for Llama 3.1\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    warmup_steps=CONFIG['warmup_steps'],\n",
        "    num_train_epochs=CONFIG['num_train_epochs'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=CONFIG['logging_steps'],\n",
        "    eval_steps=CONFIG['eval_steps'],\n",
        "    eval_strategy=\"steps\",\n",
        "    save_steps=CONFIG['save_steps'],\n",
        "    save_strategy=\"steps\",\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    optim=\"adamw_8bit\",  # Memory efficient optimizer\n",
        "    lr_scheduler_type=CONFIG['lr_scheduler_type'],\n",
        "    seed=3407,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"wandb\" if CONFIG['use_wandb'] else None,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=CONFIG['save_total_limit'],\n",
        "    logging_first_step=True,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_drop_last=False,\n",
        "    # Additional settings for stability\n",
        "    max_grad_norm=1.0,  # Gradient clipping\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_ratio=0.03,\n",
        ")\n",
        "\n",
        "# Setup SFT trainer for Llama 3.1 - SIMPLIFIED\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,  # Use raw dataset, not preprocessed\n",
        "    eval_dataset=eval_dataset,    # Use raw dataset, not preprocessed\n",
        "    dataset_text_field=\"text\",    # Make sure this matches your dataset column name\n",
        "    max_seq_length=CONFIG['max_seq_length'],\n",
        "    args=training_args,\n",
        "    packing=False,  # Keep packing disabled for stability\n",
        "    # Remove the custom data_collator - let SFTTrainer handle it\n",
        ")\n",
        "\n",
        "# Calculate training estimates\n",
        "effective_batch_size = CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']\n",
        "total_steps = len(train_dataset) // effective_batch_size * CONFIG['num_train_epochs']\n",
        "estimated_time = total_steps * 3 // 60  # Conservative estimate for 8B model\n",
        "\n",
        "print(f\"‚úÖ Training setup complete for Llama 3.1 8B!\")\n",
        "print(f\"üìä Effective batch size: {effective_batch_size}\")\n",
        "print(f\"üìä Total training steps: {total_steps:,}\")\n",
        "print(f\"üìä Estimated training time: ~{estimated_time} minutes\")\n",
        "print(f\"üìä Using precision: {'BF16' if is_bfloat16_supported() else 'FP16'}\")\n",
        "print(f\"üìä Memory optimization: 4-bit + LoRA\")\n",
        "\n",
        "# Clear memory before training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nüöÄ Starting Llama 3.1 8B Instruct training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Start training\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ Llama 3.1 8B training completed successfully!\")\n",
        "print(f\"üìä Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"‚è±Ô∏è Training time: {training_time/60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "c78232a74ef74a82b6e1b706408af4bc",
            "7f130931ae46481ea30da7446240dde2",
            "1a5ec131f9a94ef7be7a657a0b976059",
            "9c723aa740e74e41811d9796891c908a",
            "e9d45f6797414c18b3635a3c9c87731c",
            "8b513de927c1426b8cf3bedc7dcb1450",
            "550f326cae164fffadf03a18d91a27a6",
            "051b406e959d40958db1aa1c1ef49f50",
            "ac96975566e5487eac6dceaacaab9c1d",
            "3575cba28ac240689e43b44b2f4cfce6",
            "dab2422bfe904098aa5d02ffc86a9228",
            "259e02543da54a2ab3ae922a513b0984",
            "b5b405e8e4874621b61b5ddfc96d40d4",
            "567b76f6e91240ea97f2b514e3694627",
            "239d0a460499454b87e684f0c19edb7f",
            "8e9ffed15a604763bf24cfa217b856e3",
            "55b76c50895d4747af00c31f790edbe7",
            "66437631be664d5bb17c42b0e5f0ab6c",
            "cdaae0ca546d4df3ab1cc57b084f1a27",
            "d79b21d532774503b9e6288508d73fb2",
            "b9fbc4621be3410d95c404a63d914596",
            "8fc142afabea45d28e081832b489e056",
            "fbe4ddca52c84331b2dec52c1c2f2195",
            "cc43d029901a4a0d82ee643b6ae7502c",
            "8ad7fabfe6764dc5869ef4dfdd0a5960",
            "3a3abc03e50c47df9d962dce334b941b",
            "e65b6e70760f4b2eb11d8f21261cb027",
            "7da15d41b0164801b6da803e02e4a7cb",
            "acb68340849740a6bea12111f7f78c85",
            "dc9a47b44ff948a7983c50fdf6fa9e49",
            "15c84796af474c369e8ce1a9ce5cbfe1",
            "cb74b3b8d7b045d4a0d5171a89bee943",
            "390608d6ff5b49918b4057318db92504",
            "8e54217694fb4062bee002b27e512274",
            "516d83bb246c461391526d23f4237742",
            "425d86805d80493daec804d76ad2293e",
            "cc6196627f6a4f6497f56ff666adba63",
            "8f85e5a5b05b42f9a208a4fd2f448d68",
            "2b9d08f9d668475fbc13ee8fded8f033",
            "6b31224113334a57890c6f17d0d9a010",
            "cac59858854a4cedb4984dc0023ec92b",
            "48b195ef701a48818905a3eaf7240b4b",
            "a91d31df2f7b41d0829f0fc81e7154b3",
            "e16c1b226c6a4a98b26063f140860c97",
            "5d5a613115214872a97fa07749c18b06",
            "5f34efa1c31e445886de7de99bb4d6ff",
            "3ed866dd532f429b8e518944ce2c4379",
            "37c71365a61f40009d2e2fb64d32c051",
            "e332189843c1447d8ebe055ad058158f",
            "1d12015f35d24470b92b20aa6708d666",
            "1981d154701642758e3c4ae093d4d96d",
            "12d475c5327442d5ad135d8c1a4386f3",
            "3cff182c5c0f441abef68d7b4b527059",
            "2f63eb8932de48a8bde1bca346bf5404",
            "00a6035099724f81b3b5d8ddb6c98ade",
            "6f7ae7fb2a2a47d997f15302824f8b36",
            "0d2fbf1d1d6949d591fad347d5687634",
            "c301243854234b83b090a95433cf05ff",
            "07992f7c0a44409a92f518e2c223e09d",
            "a312aef597f44e8eb32f70f1be05070f",
            "e859130dfb584321bbbd64efcef92272",
            "5465d03af1d24dd1aabcb04382e0ec2d",
            "8ab284b699cf4664927bffa19cad6f1b",
            "5481958e222c4e6fa203a8454adebe2b",
            "9f6b26ec68bb45cd8a3b4b8d50434b4c",
            "b91059c8d9e24c35a7a19740df4b7135"
          ]
        },
        "id": "YqaMkFYElhpl",
        "outputId": "3f9733a4-b9a4-4c79-f97e-2251d4c64d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Trying alternative save method with smaller file chunks...\n",
            "üíæ Attempting save to: /content/drive/MyDrive/llama_31_therapist_outputs/llama31_merged_v104952\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n",
            "Downloading safetensors index for unsloth/Meta-Llama-3.1-8B-Instruct...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c78232a74ef74a82b6e1b706408af4bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "259e02543da54a2ab3ae922a513b0984",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbe4ddca52c84331b2dec52c1c2f2195",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Merging weights into 16bit:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:37<01:52, 37.64s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e54217694fb4062bee002b27e512274",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Merging weights into 16bit:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [01:36<01:39, 49.95s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d5a613115214872a97fa07749c18b06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Merging weights into 16bit:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [02:21<00:47, 47.88s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f7ae7fb2a2a47d997f15302824f8b36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:28<00:00, 37.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Save with smaller shards completed!\n"
          ]
        }
      ],
      "source": [
        "print(\"üîÑ Trying alternative save method with smaller file chunks...\")\n",
        "\n",
        "# Try saving with smaller shards to avoid large file upload issues\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%H%M%S\")\n",
        "new_save_path = f\"/content/drive/MyDrive/llama_31_therapist_outputs/llama31_merged_v{timestamp}\"\n",
        "\n",
        "print(f\"üíæ Attempting save to: {new_save_path}\")\n",
        "\n",
        "try:\n",
        "    # Save with smaller max shard size to avoid large file issues\n",
        "    model.save_pretrained_merged(\n",
        "        new_save_path,\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\",\n",
        "        max_shard_size=\"2GB\"  # Smaller chunks instead of 5GB files\n",
        "    )\n",
        "    print(\"‚úÖ Save with smaller shards completed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Save failed with error: {e}\")\n",
        "    print(\"Let's try 4-bit method instead...\")\n",
        "\n",
        "    # Fallback to 4-bit (smaller files)\n",
        "    try:\n",
        "        model.save_pretrained_merged(\n",
        "            f\"{new_save_path}_4bit\",\n",
        "            tokenizer,\n",
        "            save_method=\"merged_4bit\"\n",
        "        )\n",
        "        print(\"‚úÖ 4-bit save completed!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå 4-bit save also failed: {e2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxXZrsS9S96B",
        "outputId": "7afae19f-6e39-4984-8e01-c6a3fc8dd6f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Verifying the new save...\n",
            "==================================================\n",
            "üìÅ Model directory: /content/drive/MyDrive/llama_31_therapist_outputs/llama31_merged_v104952\n",
            "üî¢ Safetensors model files found: 4\n",
            "\n",
            "üìã File verification:\n",
            "  ‚úÖ model-00001-of-00004.safetensors: 4.63 GB\n",
            "  ‚úÖ model-00002-of-00004.safetensors: 4.66 GB\n",
            "  ‚úÖ model-00003-of-00004.safetensors: 4.58 GB\n",
            "  ‚úÖ model-00004-of-00004.safetensors: 1.09 GB\n",
            "\n",
            "üíæ Total model size: 14.96 GB\n",
            "\n",
            "üéâ SUCCESS: All model files appear to be present!\n",
            "üöÄ Your fine-tuned model should be complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check the new timestamped model folder\n",
        "model_path = \"/content/drive/MyDrive/llama_31_therapist_outputs/llama31_merged_v104952\"\n",
        "\n",
        "print(\"üîç Verifying the new save...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    files = os.listdir(model_path)\n",
        "\n",
        "    # Look for the model files\n",
        "    safetensors_files = [f for f in files if f.endswith('.safetensors') and 'model-' in f]\n",
        "\n",
        "    print(f\"üìÅ Model directory: {model_path}\")\n",
        "    print(f\"üî¢ Safetensors model files found: {len(safetensors_files)}\")\n",
        "    print()\n",
        "\n",
        "    # Check each file and its size\n",
        "    print(\"üìã File verification:\")\n",
        "    total_size_gb = 0\n",
        "\n",
        "    for file in sorted(safetensors_files):\n",
        "        file_path = os.path.join(model_path, file)\n",
        "        if os.path.exists(file_path):\n",
        "            size_gb = os.path.getsize(file_path) / (1024**3)\n",
        "            total_size_gb += size_gb\n",
        "            print(f\"  ‚úÖ {file}: {size_gb:.2f} GB\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå {file}: MISSING!\")\n",
        "\n",
        "    print(f\"\\nüíæ Total model size: {total_size_gb:.2f} GB\")\n",
        "\n",
        "    if len(safetensors_files) >= 4:\n",
        "        print(\"\\nüéâ SUCCESS: All model files appear to be present!\")\n",
        "        print(\"üöÄ Your fine-tuned model should be complete!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Found {len(safetensors_files)} files\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Model directory not found: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJB-EWuGllXV",
        "outputId": "a7fcd56e-20ea-469a-b1c1-8d9a91a5ddd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Loading test dataset (NickyNicky/nlp-mental-health-conversations)...\n",
            "üß™ Generating responses on test dataset...\n",
            "Processing test sample 1/10...\n",
            "Processing test sample 2/10...\n",
            "Processing test sample 3/10...\n",
            "Processing test sample 4/10...\n",
            "Processing test sample 5/10...\n",
            "Processing test sample 6/10...\n",
            "Processing test sample 7/10...\n",
            "Processing test sample 8/10...\n",
            "Processing test sample 9/10...\n",
            "Processing test sample 10/10...\n",
            "\n",
            "‚úÖ Test responses saved to: /content/drive/MyDrive/llama_31_therapist_outputs/test_responses_nickynicky.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "print(\"üß™ Loading test dataset (NickyNicky/nlp-mental-health-conversations)...\")\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = load_dataset(CONFIG['test_dataset_name'])\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"üß™ Generating responses on test dataset...\")\n",
        "\n",
        "# Sample a subset for testing (to avoid overwhelming output)\n",
        "test_samples = test_dataset['train'].shuffle(seed=999).select(range(10))  # Test on 10 samples\n",
        "\n",
        "responses = []\n",
        "for i, example in enumerate(test_samples):\n",
        "    context = example.get('Context', '').strip()\n",
        "    expected_response = example.get('Response', '').strip()\n",
        "\n",
        "    print(f\"Processing test sample {i+1}/10...\")\n",
        "\n",
        "    # Format prompt specifically for Llama 3.1 Instruct\n",
        "    formatted_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful, empathetic mental health assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=CONFIG['generation_config']['max_new_tokens'],\n",
        "            temperature=CONFIG['generation_config']['temperature'],\n",
        "            do_sample=CONFIG['generation_config']['do_sample'],\n",
        "            top_p=CONFIG['generation_config']['top_p'],\n",
        "            repetition_penalty=CONFIG['generation_config']['repetition_penalty'],\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response\n",
        "    assistant_response = full_response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
        "\n",
        "    responses.append({\n",
        "        'context': context,\n",
        "        'expected_response': expected_response,\n",
        "        'generated_response': assistant_response,\n",
        "        'model': 'Llama-3.1-8B-Instruct-Therapist'\n",
        "    })\n",
        "\n",
        "# Save test responses\n",
        "with open(f\"{CONFIG['output_dir']}/test_responses_nickynicky.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(responses, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Test responses saved to: {CONFIG['output_dir']}/test_responses_nickynicky.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk8VSzcRkfve",
        "outputId": "b2e0036f-59dd-4a16-82ea-54263c3785c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Test Sample 1 ===\n",
            "Context: I keep having these random thoughts that I don't want.  Things like \"you aren't worth anything.\"  I know they're my own thoughts but it feels like someone else is saying it.\r\n",
            "\r\n",
            "What is wrong with me, and how can I stop having these thoughts?\n",
            "\n",
            "Expected: Talking to a licensed profession who can discuss this in greater depth can be best.¬†As a general information, in short, I can say that our thoughts are greatly influenced¬†by our early life experiences. Our thoughts are processed through schemes, these are mental images or templates by which we make meaning of the world around us. ¬†While our upbringing has a great influence on the way we see and interact with the world around us as adults, However, we are not condemned to abide by them for life, in psychotherapy, you learn to change negative¬†schemas with positive ones. Yeah, if you had less than optimal childhood you would have some sort of negative schemas that unconsciously lead to self-sabotage your efforts for success and happiness. The research in the field of interpersonal neurobiology suggests that without conscious awareness and reflective practices we tend to interact with the world by repeating old habits of mind. On the other hand, in psychotherapy you can learn helpful¬†strategies to increase your conscious abilities to stay in control of your mind in the present moment and reduce intrusions of negative mind habits.¬†Yoga, meditation, and tai¬†chi are also found to increase self-awareness and lessen the intrusions of negative self-judgment on one's¬†psyche.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "I keep having these random thoughts that I don't want.  Things like \"you aren't worth anything.\"  I know they're my own thoughts but it feels like someone else is saying it.\r\n",
            "\r\n",
            "What is wrong with me, and how can I stop having these thoughts?assistant\n",
            "\n",
            "It's understandable that you're feeling concerned about the recurring negative thoughts you've been experiencing, especially when they seem to be your own thoughts. It could be helpful for us to explore this further.\n",
            "\n",
            "Firstly, it might be beneficial to recognize that these thoughts do not define who you are as a person. They may stem from past experiences or self-critical beliefs that have taken hold in your mind. Acknowledging this can help separate yourself from the thoughts themselves.\n",
            "\n",
            "One approach we can consider is challenging the validity of these thoughts. Take some time to reflect on whether there is any evidence supporting them. Often, our inner critic distorts reality by emphasizing only the negative aspects while ignoring positive ones. By questioning their accuracy, we can begin to gain perspective and counteract their impact.\n",
            "\n",
            "Another strategy is practicing mindfulness techniques such as deep breathing exercises or meditation. These practices can assist in grounding yourself in the present moment and reducing the influence of intrusive thoughts. Regular practice can also enhance overall emotional well-being and resilience.\n",
            "\n",
            "Additionally, engaging in activities that bring you joy and fulfillment can contribute positively to your mindset. This might include hobbies, spending time with loved ones, pursuing personal goals, or seeking support through therapy if desired.\n",
            "\n",
            "Remember, change takes time, so try not to get discouraged if progress isn't immediate. Each step forward counts towards building a healthier relationship with your thoughts. You deserve compassion and understanding throughout this journey.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 2 ===\n",
            "Context: My mom made a lot of mistakes a couple years back, and I can't seem to forgive her. I want to say that it's okay, but I can't. I feel like even if I did forgive her, I wouldn't be able to say it to her face.\n",
            "\n",
            "Expected: Hi Cleveland, I think I get what you're feeling. You're scared that if you forgive your mom, that would mean it wasn't so bad, or she'll think it's okay to do it again, or she won't realize how hurt you are.¬†I invite you to look at forgiveness in a different way. Forgiveness doesn't have to mean what she did was okay. Not at all. Forgiveness can mean \"I accept what she did, and I don't want to live in her worst moments constantly. I want to put it down and move past it because I want her in my life, because fear and resentment interfere with my relationship with her, and mostly because I'm tired of carrying them (fear and resentment) around constantly.\"You can forgive your mom for yourself, not for her.¬†¬†If there is evidence that your mom wants to respect you moving forward, then it might be worth it to risk putting down the resentment and the grudge that you think you need to protect yourself.¬†¬†Your ability to draw boundaries with her in the present is your biggest protector; let her know in the present what hurts you, and notice her responses.You don't have to say it to her face. Saying it to yourself is more important. Again, forgiving your mom helps you...it reduces your own burden of fear and resentment, leaving space for better things.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "My mom made a lot of mistakes a couple years back, and I can't seem to forgive her. I want to say that it's okay, but I can't. I feel like even if I did forgive her, I wouldn't be able to say it to her face.assistant\n",
            "\n",
            "I understand how difficult it is to forgive your mom for the mistakes she made in the past. It's natural to feel angry, hurt, and confused by her actions. However, holding onto these feelings can prevent you from healing and moving forward.\n",
            "\n",
            "Your mom might not even realize the impact her actions had on you. She may be struggling with her own emotions and dealing with her own pain. It's important to remember that everyone makes mistakes, and it doesn't define who they are as a person.\n",
            "\n",
            "Forgiveness isn't about forgetting what happened or condoning your mom's behavior. Instead, it's about letting go of the anger and resentment you feel towards her. This process can take time, and it's essential to be patient with yourself.\n",
            "\n",
            "One approach to forgiveness is practicing self-compassion. This means treating yourself with kindness and understanding when you're feeling upset or overwhelmed. You can also try writing down your feelings about your mom's mistakes and then tearing up the paper or setting it aside. This act of releasing the negative emotions can help you begin the healing process.\n",
            "\n",
            "Additionally, consider having an open and honest conversation with your mom about how her actions affected you. Expressing your feelings in a calm and respectful manner can help both of you better understand each other's perspectives.\n",
            "\n",
            "Remember, forgiveness is a journey, and it's not always easy. But with patience, self-compassion, and open communication, you can work towards healing and finding peace within yourself.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 3 ===\n",
            "Context: What should I do when we see each other?\n",
            "\n",
            "Expected: I am so sorry that this happened. ¬†Nobody deserves to be outed without their permission. ¬†I would encourage you to get in touch with someone who is supportive and accepting and plan to touch base with them after you see your wife's sister. ¬†You can also plan to take some time for self care after you see her. ¬†For example, if you enjoy bike rides, plan on taking a bike ride afterward. ¬†Plan to do something that feels nurturing and caring. ¬†Best of luck!\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "What should I do when we see each other?assistant\n",
            "\n",
            "When you see your partner after a long time apart, it's essential to approach the situation with care and consideration for both of your emotional well-being. Here are some suggestions on what you could do:\n",
            "\n",
            "1. Take your time: Allow yourself and your partner to adjust to being together again slowly. Rushing into intense conversations or expectations might put unnecessary pressure on the relationship.\n",
            "\n",
            "2. Communicate openly: Set aside dedicated time to talk about how you've been feeling during your separation and any concerns or changes you may have noticed in yourselves or the relationship. Open communication helps build trust and understanding.\n",
            "\n",
            "3. Prioritize quality time: Plan activities that allow you both to reconnect emotionally and physically. This could involve going for walks together, cooking meals as a team, watching movies, or engaging in shared hobbies. Focus on creating positive experiences rather than dwelling on past issues.\n",
            "\n",
            "4. Be patient with each other: Recognize that adjusting to living together after a period apart can be challenging. Both partners may need time to readjust and rebuild their connection. Patience is crucial during this process.\n",
            "\n",
            "5. Practice self-care: Taking care of your own well-being will benefit not only you but also your relationship. Engage in activities that bring you joy and help reduce stress. Self-care sets the foundation for healthier interactions within the partnership.\n",
            "\n",
            "6. Seek support if needed: If you find it difficult to navigate these emotions independently or notice significant strain in the relationship, consider seeking professional guidance from couples therapists who specialize in rebuilding relationships. They can provide valuable insights and strategies tailored to your unique circumstances.\n",
            "\n",
            "Remember, every couple is different, so it's important to adapt these suggestions based on your individual needs and dynamics. Rebuilding a relationship takes time, effort, and mutual commitment.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 4 ===\n",
            "Context: I'm always listening to my husband, but it feels like he never listens to me.    How do I get my husband to listen to me instead of me listening to him all the time?\n",
            "\n",
            "Expected: It's hard when you feel as if you're the only one that's taking the time to listen to your spouse. But, I would look at this as an opportunity to see if you can become aware of what exactly is happening between you, when you try and talk with your husband.¬†Sometimes, it can be in the way dialogue is approached. I would suggest paying attention to the way you begin dialogue with your husband. See if blame and criticism are present. When blame and criticism are included, bids for connection, can quickly go off track. This can sometimes start off with something like: \"why don't you...\" \"you aren't...\" \"you don't...\" Partners can quickly go into defensive mode if they feel they are being attacked and sometimes starting off like this can feel like an attack. ¬†Also, become aware of the time of day or evening when you approach your husband. Sometimes, this can make a big difference for couples as far as when they can truly be present for one another. ¬†If you find this pattern continues, you might consider seeking professional help through couples therapy. A trained couples therapist can help you both understand more about what's happening between you.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "I'm always listening to my husband, but it feels like he never listens to me.    How do I get my husband to listen to me instead of me listening to him all the time?assistant\n",
            "\n",
            "It can be frustrating when we feel like our partner isn't actively hearing and understanding us in a relationship. Communication is key to addressing this issue, so here are some suggestions that might help:\n",
            "\n",
            "1. Start by expressing your feelings: Share with your husband how you've been feeling lately. Be specific about what specifically makes you feel unheard or unacknowledged. This will give him an opportunity to understand your perspective.\n",
            "\n",
            "2. Use \"I\" statements: When discussing sensitive topics, try using \"I\" statements rather than placing blame or making accusations. For example, say something like, \"I feel hurt when I don't receive validation for my thoughts and emotions.\" This approach encourages open dialogue without escalating tensions.\n",
            "\n",
            "3. Active listening: Encourage your husband to practice active listening during conversations. Remind him to maintain eye contact, nodding, and provide verbal cues to show engagement. Letting him know that his presence and attention matter to you may motivate him to make more effort.\n",
            "\n",
            "4. Schedule dedicated conversation time: Set aside regular periods where both of you can focus solely on each other without distractions. Create an environment conducive to deep discussions, ensuring both partners have equal opportunities to speak freely and be heard.\n",
            "\n",
            "5. Seek compromise: Remember that effective communication requires mutual respect and understanding. If your husband struggles to listen initially, consider finding common ground and agreeing to work together towards better communication practices. Compromise involves considering each other's needs while still maintaining personal boundaries.\n",
            "\n",
            "6. Reflect on your own behavior: It's essential not only to encourage your husband to improve but also reflect upon your own actions. Are there any patterns that contribute to the imbalance in listening dynamics? Self-awareness and personal growth can positively influence relationships.\n",
            "\n",
            "7. Consider seeking professional support if needed: Sometimes external guidance can offer fresh perspectives and strategies. Couples therapy can provide a safe space for both parties to express their concerns openly and learn healthier ways of communicating effectively.\n",
            "\n",
            "Remember, change takes time and patience. By promoting open dialogue and encouraging your husband to engage more actively in conversations, you're taking steps toward fostering a healthier dynamic in your relationship.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 5 ===\n",
            "Context: I've gone to a couple therapy sessions so far and still everytime I walk in I get nervous and shaky.  Is this normal? Should I still be feeling like this?\n",
            "\n",
            "Expected: of ¬†I would love to know a little bit more about what's going on in your life but I will attempt an answer.Yes, you could still be shaky and nervous going to therapy. This therapy thing your doing is sometimes scary.¬†First, because your opening up things that you might have never wanted to.¬†Second, your still building a relationship with this therapist person. You may never get over that. The therapist really can't be your \"friend\". They are there to push the buttons that you might not want pushed and help you heal. ¬†That in itself is scary and can make you anxious.¬†Third, you really never know where this therapy thing will go. Yes, there are goals. But sometimes side roads need to be taken and sometimes that is scary.Know this you are in the right place. You are taking steps to change.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "I've gone to a couple therapy sessions so far and still everytime I walk in I get nervous and shaky.  Is this normal? Should I still be feeling like this?assistant\n",
            "\n",
            "It's completely normal to feel nervous and shaky during therapy sessions, especially if it's your first time or you're new to the process. Therapy can be an unfamiliar environment for many people, and it's natural to have some anxiety about opening up and discussing personal issues.\n",
            "\n",
            "Your feelings of nervousness might stem from various factors such as fear of judgment, vulnerability, or uncertainty about what will happen during the session. It could also be influenced by past experiences where you felt uncomfortable sharing your thoughts and emotions with others.\n",
            "\n",
            "Despite these initial nerves, attending therapy is a significant step towards self-improvement and growth. The therapeutic relationship offers a safe space where you can explore your concerns, gain insights into yourself, and develop coping strategies.\n",
            "\n",
            "To manage your anxiety during therapy sessions, here are a few suggestions:\n",
            "\n",
            "1. Take slow deep breaths: Before entering the therapy room, take a moment to focus on your breathing. Inhale deeply through your nose, hold it for a second, and then exhale slowly through your mouth. This simple technique can help calm your body and mind.\n",
            "\n",
            "2. Remind yourself that therapists are there to support you: Therapists are trained professionals who work collaboratively with their clients to create a supportive atmosphere. They are not there to judge or criticize you but rather to provide guidance and facilitate healing.\n",
            "\n",
            "3. Set small goals for each session: Break down larger topics into smaller, manageable parts. By focusing on one specific issue per session, you may find it easier to engage in the conversation without feeling overwhelmed.\n",
            "\n",
            "4. Practice self-compassion: Be kind to yourself throughout the process. Understand that it takes courage to seek help and acknowledge areas in need of improvement. Remember that progress often happens gradually over time, so try not to rush things.\n",
            "\n",
            "5. Communicate openly with your therapist: Letting your therapist know how you're feeling before or during the session can greatly enhance your experience. They can offer reassurance, adjust their approach to better suit your needs, and ensure that you feel comfortable throughout the process.\n",
            "\n",
            "Remember, therapy is a journey filled with ups and downs. While initial nervousness is common, your commitment to attending these sessions demonstrates strength and resilience. Keep pushing forward at your own pace, and don't hesitate to communicate any concerns or challenges along the way. Your therapist will guide you through this process, helping you navigate the complexities of your emotional landscape.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 6 ===\n",
            "Context: How do you know you have the right therapist for you?\n",
            " How would I know how to \"train\" my therapist to be able to give me what I need from treatment?\n",
            "\n",
            "Expected: Counselors do not expect to gain your trust during the first session. Trust is earned and gained through the therapeutic process. You may know you have the right therapist if you feel a lack of judgement or even unconditional positive regard for the choices you make in therapy. Good counselors will keep you accountable for your actions without making you feel ashamed of the choice you made.Counselors already have the training to give you what you need in treatment, and if they don't they are ethically obligated to refer you to a provider that does. That being said, the first few visits with your counselor will be goal oriented- creating realistic and obtainable goals that will allow you and your therapist to see positive change when it is made.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "How do you know you have the right therapist for you?\n",
            " How would I know how to \"train\" my therapist to be able to give me what I need from treatment?assistant\n",
            "\n",
            "Finding the right therapist can be a personal and subjective process, as it's important to find someone who aligns with your needs, values, and therapeutic goals. Here are some suggestions on how to determine if you've found the right therapist for you:\n",
            "\n",
            "1. Trust and rapport: A strong therapeutic relationship is built upon trust and rapport between you and your therapist. If you feel comfortable opening up about sensitive topics and believe that your therapist genuinely cares about your well-being, it could indicate a good fit.\n",
            "\n",
            "2. Understanding and validation: Your therapist should strive to understand your unique experiences and emotions without judgment or bias. They should validate your feelings and provide an environment where you feel heard and supported. Reflecting back your thoughts and concerns in a way that feels authentic may signal a positive therapeutic alliance.\n",
            "\n",
            "3. Communication style: The way your therapist communicates their approach, explanations of techniques, and expectations should resonate with you. Pay attention to whether they listen attentively, ask insightful questions, and clarify any uncertainties you might have. Feeling understood and informed throughout therapy sessions is crucial.\n",
            "\n",
            "4. Goals alignment: It's essential to discuss your therapeutic objectives and ensure that both you and your therapist share similar aspirations. Identifying common goals helps establish a clear direction for your treatment and ensures progress toward achieving them.\n",
            "\n",
            "5. Ethical conduct: Look for therapists who operate within ethical boundaries, maintaining confidentiality, respecting your autonomy, and avoiding dual relationships. Trusting that your therapist prioritizes your best interests fosters a safe space for healing.\n",
            "\n",
            "Regarding training your therapist, it's important to remember that therapists undergo extensive education and training to develop their skills. However, there are still ways to engage actively during therapy sessions to enhance the effectiveness of the treatment:\n",
            "\n",
            "1. Open communication: Share your thoughts, concerns, preferences, and expectations with your therapist openly. Honest dialogue allows them to tailor their approach according to your specific needs.\n",
            "\n",
            "2. Clarify instructions: If you're unsure about something related to treatment, don't hesitate to ask clarifying questions. Clear understanding enables both parties to work collaboratively towards desired outcomes.\n",
            "\n",
            "3. Feedback and adjustments: Provide constructive feedback during therapy sessions. Letting your therapist know what has been helpful or suggesting alternative strategies gives them an opportunity to adjust their approach based on your responses.\n",
            "\n",
            "4. Active participation: Engage fully in therapy by being present, sharing your insights honestly, and taking responsibility for your own growth. Actively participating demonstrates your commitment to the process, which often leads to more effective results.\n",
            "\n",
            "Remember, finding the right therapist is a journey, and it\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 7 ===\n",
            "Context: I am in my early 20s and I still live with my parents because I can't afford to live alone.\r\n",
            "\r\n",
            "My mother says that if I live under her roof I have to follow her rules. She is trying to control my life. What should I do?\n",
            "\n",
            "Expected: Unfortunately you seem to have yourself in a double bind. By living with your mom she is not going to stop her attempts at what you perceive as \"controlling\". There is no use in trying to debate or get her to \"see your point\". As long as your there and you personalize her attempts at control, then you will find yourself frustrated and resentful. Try to find an extra job or a roommate so that you can do the natural process of moving on out.www.lifecounselingorlando.com\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "I am in my early 20s and I still live with my parents because I can't afford to live alone.\r\n",
            "\r\n",
            "My mother says that if I live under her roof I have to follow her rules. She is trying to control my life. What should I do?assistant\n",
            "\n",
            "Living with your parents can be challenging, especially when they impose rules on you that you feel uncomfortable with. It's important to remember that open communication and setting boundaries are key steps towards finding a resolution.\n",
            "\n",
            "Firstly, try having an honest conversation with your mother about how their rules affect you emotionally and mentally. Expressing your feelings calmly and assertively may help her understand why these restrictions cause distress for you. This approach might lead to a more constructive discussion rather than escalating into conflict.\n",
            "\n",
            "Additionally, consider exploring alternative living arrangements that would allow you greater independence while still being financially stable. Researching affordable housing options or shared accommodations could provide opportunities for personal growth and autonomy without completely cutting ties with your family.\n",
            "\n",
            "While it may not always be easy, maintaining healthy boundaries within your relationship with your parents is crucial. Setting clear expectations regarding personal space, privacy, and decision-making processes can promote mutual respect and understanding.\n",
            "\n",
            "In the meantime, focusing on self-care activities such as exercise, hobbies, or joining social groups outside of home can offer emotional support and distract from any tensions between you and your mother. Remember, taking care of yourself is essential during this period.\n",
            "\n",
            "Lastly, seeking guidance from trusted friends or mentors who have faced similar situations might provide valuable insights and perspectives. Sometimes talking things out with someone who has gone through something comparable can give us new ideas on how to navigate our own challenges effectively.\n",
            "\n",
            "Remember, change takes time and effort. Be patient with yourself and remain committed to finding solutions that align with your values and aspirations. You deserve happiness and fulfillment, both personally and professionally. Keep striving forward!\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 8 ===\n",
            "Context: I think i may suffer from depression, and it is affecting my life and sleep. I am on my parent's insurance, and they won't let me see a therapist because they don't believe in it.\n",
            "\n",
            "Expected: The answer here really depends upon your age and something called the \"age of consent\" in the state where you live. In Pennsylvania, anyone over the age of 14 can provide consent for their own treatment. As for billing purposes, try talking to some local agencies. If you are under 18, you may be able to have Medicaid out of medical necessity so that you can have treatment. If you are over 18, there may be some discounted rates at some agencies because of the situation you're in. Maybe your primary care physician will be able to refer you to some local agencies.If you are in school, you may be able to talk with a guidance counselor or someone at the school. This would not require using your insurance.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "I think i may suffer from depression, and it is affecting my life and sleep. I am on my parent's insurance, and they won't let me see a therapist because they don't believe in it.assistant\n",
            "\n",
            "It sounds like you're going through a tough time right now, and I can understand how frustrating it must be for you to not have access to the support you need due to your parents' beliefs about therapy. It's important to remember that seeking help for mental health issues is just as valid and necessary as any other medical treatment.\n",
            "\n",
            "In situations like this, there are still ways for you to explore potential options:\n",
            "\n",
            "1. Self-reflection: Take some time to reflect on your feelings and experiences. Journaling or writing down your thoughts might provide insight into what triggers your depressive episodes and how they impact different areas of your life.\n",
            "2. Support groups: Look for online or local support groups where you can connect with others who may be experiencing similar challenges. Sharing your story and hearing from people who understand can offer valuable emotional support.\n",
            "3. Educational resources: Educate yourself about depression by reading books, articles, or reputable websites. This knowledge will empower you to make informed decisions regarding your own well-being.\n",
            "4. Alternative therapists: While traditional talk therapy might not be an option at the moment, consider alternative approaches such as art therapy, music therapy, or mindfulness exercises. These methods can also aid in managing symptoms and promoting overall wellness.\n",
            "5. Reach out to trusted individuals: Talk to friends, family members, or teachers whom you trust about what you've been going through. Sometimes, having someone listen without judgment can alleviate the weight of your emotions.\n",
            "6. Online counseling platforms: Some organizations offer affordable online counseling services that could potentially provide the assistance you seek while maintaining privacy and confidentiality.\n",
            "7. Advocacy: If you feel comfortable doing so, discuss your concerns with your parents gently yet firmly explaining why therapy is essential for your well-being. You can share stories of successful cases or statistics highlighting the effectiveness of therapy in treating various mental health conditions.\n",
            "\n",
            "Remember, finding appropriate help takes time, but please don't lose hope. Keep exploring avenues until you find one that works best for you. And most importantly, know that you deserve compassion and support during this difficult journey.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 9 ===\n",
            "Context: I use to be so happy. No matter what, I always was happy. I got into a relationship with this guy. I love him so much. We‚Äôre both teenagers. The week after his birthday, my mom made me stop talking to him. It broke me. He came to my house and talked to her, and she let us date again but not see each other. He comes up to my school every day and it tears me apart that I have to lie to her.\n",
            "\n",
            "Expected: Hi Los Angeles, I can tell you have a moral backbone because it's hurting you to lie to your mom. I always say that a little bit of guilt is healthy because it teaches us who we want to be. Listen to that voice. You don't have to lie to your mom to get what you want.Now, you don't say how old you are, so part of my answer is going to be a bit vague. There's a big difference between being thirteen and your mom telling you what to do and being nineteen and your mom telling you what to do. The thing is that moms and dads start out making all your decisions for you when you're small, and they're supposed to, very gradually, give that control over to you. Different parents do this at different speeds, but in general you have to earn that freedom and trust...it's not just gonna be handed over to you.¬†You might not like this part...your parent has the right to make the rules. My guess is there's a reason why mom is holding you back. Even if it doesn't make sense to you, breaking her rules isn't going to get you what you want...which is freedom, right? I guarantee you that mom wants you to eventually earn freedom...but she's not supposed to give it to you until you're ready because her first job is to keep you safe.¬†I like that your mom has limits for you. Too many parents are sort of looking the other way, busy with their own thing. Whether you know it or not, you need protection like all kids do (appropriate for your age, of course).It sounds like what you want to say to your mom is \"I want to make my own decisions about who I spend time with\". That's a normal wish, and I encourage you to say that to her. Depending on your age and whether you've earned it, she might listen.¬†So, you're sending her that message that you want to send, but you're doing it in a sneaky way, and that's not going to go well when she finds out. It's called \"passive aggressive\" when you say something with a sneaky behaviour rather than with words. It's not a very healthy way of sending a message, because people feel disrespected.¬†My guess is that your best bet is to be honest with mom, stop the sneaking around and come up with a detailed plan of how you're going to earn your freedom...eventually. She might listen to that.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "I use to be so happy. No matter what, I always was happy. I got into a relationship with this guy. I love him so much. We‚Äôre both teenagers. The week after his birthday, my mom made me stop talking to him. It broke me. He came to my house and talked to her, and she let us date again but not see each other. He comes up to my school every day and it tears me apart that I have to lie to her.assistant\n",
            "\n",
            "Your story is one of heartache and longing. You've shared that you used to be content with your life, but the introduction of this new relationship has brought about significant turmoil. Your mother's disapproval has left you feeling torn between your loyalty to her and your deep affection for this young man.\n",
            "\n",
            "The situation at hand is complex, as it involves balancing your emotions, relationships, and personal growth. It's essential to acknowledge the pain you're experiencing and validate your feelings. However, it's also crucial to consider the potential consequences of continuing to deceive your mother while maintaining a secret relationship.\n",
            "\n",
            "As we delve deeper into this issue, we'll explore various strategies to help you navigate this challenging situation. This may include open communication with your mother, seeking alternative ways to maintain your relationship, or considering the long-term implications of your choices.\n",
            "\n",
            "Throughout our sessions, we'll work together to develop coping skills, such as mindfulness practices, journaling, and cognitive restructuring techniques. These tools will enable you to manage your emotions more effectively and make informed decisions based on a clear understanding of your values, goals, and priorities.\n",
            "\n",
            "Ultimately, our goal will be to help you find a path forward that respects your autonomy, preserves your emotional well-being, and fosters healthy relationships in all aspects of your life.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "=== Test Sample 10 ===\n",
            "Context: I know that I need to get past my feelings for this person I fell in love with, but t's so difficult to move on because he showed me feelings I've never felt before.    I feel like I don't want to be without my genuine love for him, but logically, I know I need to be without him. I can't discuss this with anybody in my life because the conditions surrounding our love are considered \"wrong\" by a lot of people.\n",
            "   How can I get myself to just move on?\n",
            "\n",
            "Expected: There is no wrong or right way to define a relationship. I believe each relationship we are in is an opportunity to expand and to know self on a deeper level. We are conditioned to believe that we are not valued, or worth much without the confirmation of others and the world around us.¬†Give yourself time and try to not go to those who are toxic and enjoy the drama of others lives as s way to avoid looking at themselves.\n",
            "\n",
            "Generated: system\n",
            "\n",
            "You are a helpful, empathetic mental health assistant.user\n",
            "\n",
            "I know that I need to get past my feelings for this person I fell in love with, but t's so difficult to move on because he showed me feelings I've never felt before.    I feel like I don't want to be without my genuine love for him, but logically, I know I need to be without him. I can't discuss this with anybody in my life because the conditions surrounding our love are considered \"wrong\" by a lot of people.\n",
            "   How can I get myself to just move on?assistant\n",
            "\n",
            "It sounds like you're going through a really challenging time right now. Falling in love with someone is a powerful experience, and it's natural to feel attached to them even after they're no longer present in your life.\n",
            "\n",
            "Your feelings towards this person may be intense and overwhelming, but it's important to remember that emotions are temporary and they will eventually fade over time. It might help to remind yourself that these feelings won't last forever and that there is a light at the end of this tunnel.\n",
            "\n",
            "One way to begin moving forward could be to focus on self-care. Take care of yourself physically, emotionally, and mentally. Engage in activities that bring you joy or provide a sense of accomplishment. Surround yourself with positive influences - whether it's spending time with loved ones who support you, engaging in hobbies, or seeking out new interests that inspire you.\n",
            "\n",
            "Another strategy could involve finding healthy outlets for expressing your emotions. Journaling can be an effective tool for processing thoughts and feelings, allowing you to gain clarity and perspective. Consider talking to trusted friends or family members about what you're going through, if they are open and willing to listen. Sometimes simply verbalizing our emotions can help alleviate their intensity.\n",
            "\n",
            "Additionally, practicing mindfulness techniques such as deep breathing exercises or meditation can assist in grounding yourself during moments of distress. These practices can help calm racing thoughts and bring your attention back to the present moment.\n",
            "\n",
            "Remember, healing takes time and everyone goes through different phases. Be patient with yourself throughout this process and allow yourself to grieve the loss while also focusing on personal growth and moving forward.\n",
            "\n",
            "Please keep in mind that these suggestions are not meant to replace professional advice, but rather offer guidance based on general principles of emotional well-being. If you find yourself struggling significantly or feeling overwhelmed, reaching out to a mental health expert would still be beneficial.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Read and display the file\n",
        "with open(f\"{CONFIG['output_dir']}/test_responses_nickynicky.json\", 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Pretty print the results\n",
        "for i, response in enumerate(data):\n",
        "    print(f\"\\n=== Test Sample {i+1} ===\")\n",
        "    print(f\"Context: {response['context']}\")\n",
        "    print(f\"\\nExpected: {response['expected_response']}\")\n",
        "    print(f\"\\nGenerated: {response['generated_response']}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFAx92cylsVP",
        "outputId": "b867465d-0b28-44c1-c55e-cb9a1432bea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üéâ LLAMA 3.1 8B THERAPIST TRAINING COMPLETED! üéâ\n",
            "============================================================\n",
            "ü¶ô Model: Llama 3.1 8B Instruct\n",
            "üìö Training dataset: ShenLab/MentalChat16K\n",
            "üß™ Testing dataset: NickyNicky/nlp-mental-health-conversations\n",
            "‚è±Ô∏è Total training time: 307.9 minutes\n",
            "üìä Final training loss: 0.7210\n",
            "üß† Trainable parameters: 41,943,040 (0.92%)\n",
            "üìÅ All files saved to: /content/drive/MyDrive/llama_31_therapist_outputs\n",
            "üîó Check your Google Drive at: /content/drive/MyDrive/llama_31_therapist_outputs/\n",
            "üíæ LoRA model: llama31_lora_model/\n",
            "üíæ Merged model: llama31_merged_v104952/\n",
            "üìä Test results: test_responses_nickynicky.json\n",
            "\n",
            "‚úÖ Memory cleared. Llama 3.1 training complete!\n"
          ]
        }
      ],
      "source": [
        "# Add the correct datetime import\n",
        "from datetime import datetime\n",
        "\n",
        "# Training summary with updated paths\n",
        "summary = {\n",
        "    'model': 'Llama-3.1-8B-Instruct',\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'training_time_minutes': training_time / 60,\n",
        "    'final_training_loss': float(trainer_stats.training_loss),\n",
        "    'total_parameters': total_params,\n",
        "    'trainable_parameters': trainable_params,\n",
        "    'trainable_percentage': 100 * trainable_params / total_params,\n",
        "    'training_samples': len(train_dataset),\n",
        "    'validation_samples': len(eval_dataset),\n",
        "    'effective_batch_size': effective_batch_size,\n",
        "    'total_training_steps': total_steps,\n",
        "    'gpu_used': torch.cuda.get_device_name(),\n",
        "    'max_sequence_length': CONFIG['max_seq_length'],\n",
        "    'learning_rate': CONFIG['learning_rate'],\n",
        "    'lora_rank': CONFIG['lora_r'],\n",
        "    'train_dataset': CONFIG['train_dataset_name'],\n",
        "    'test_dataset': CONFIG['test_dataset_name'],\n",
        "    'config': CONFIG\n",
        "}\n",
        "\n",
        "# Save to the updated output directory\n",
        "with open(f\"{CONFIG['output_dir']}/llama31_training_summary.json\", 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# Log final metrics to wandb\n",
        "if CONFIG['use_wandb']:\n",
        "    wandb.log({\n",
        "        \"training/final_loss\": trainer_stats.training_loss,\n",
        "        \"training/total_time_minutes\": training_time / 60,\n",
        "        \"training/trainable_params\": trainable_params,\n",
        "        \"training/total_params\": total_params,\n",
        "        \"training/model\": \"Llama-3.1-8B-Instruct\",\n",
        "        \"training/train_dataset\": CONFIG['train_dataset_name'],\n",
        "        \"training/test_dataset\": CONFIG['test_dataset_name'],\n",
        "        \"training/status\": \"completed\"\n",
        "    })\n",
        "\n",
        "    # Log test responses as table\n",
        "    test_table = wandb.Table(columns=[\"Context\", \"Expected\", \"Generated\"])\n",
        "    for resp in responses:\n",
        "        test_table.add_data(resp['context'], resp['expected_response'], resp['generated_response'])\n",
        "    wandb.log({\"test_responses\": test_table})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ LLAMA 3.1 8B THERAPIST TRAINING COMPLETED! üéâ\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ü¶ô Model: Llama 3.1 8B Instruct\")\n",
        "print(f\"üìö Training dataset: {CONFIG['train_dataset_name']}\")\n",
        "print(f\"üß™ Testing dataset: {CONFIG['test_dataset_name']}\")\n",
        "print(f\"‚è±Ô∏è Total training time: {training_time/60:.1f} minutes\")\n",
        "print(f\"üìä Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"üß† Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"üìÅ All files saved to: {CONFIG['output_dir']}\")\n",
        "print(f\"üîó Check your Google Drive at: /content/drive/MyDrive/llama_31_therapist_outputs/\")\n",
        "print(f\"üíæ LoRA model: llama31_lora_model/\")\n",
        "print(f\"üíæ Merged model: llama31_merged_v104952/\")  # Updated to match new naming\n",
        "print(f\"üìä Test results: test_responses_nickynicky.json\")\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\n‚úÖ Memory cleared. Llama 3.1 training complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
